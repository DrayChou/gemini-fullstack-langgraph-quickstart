#!/usr/bin/env python3
"""
éªŒè¯ç¯å¢ƒå˜é‡åŠ è½½çš„è„šæœ¬
"""

import os
from dotenv import load_dotenv

def test_env_loading():
    """æµ‹è¯•ç¯å¢ƒå˜é‡åŠ è½½"""
    print("=== ç¯å¢ƒå˜é‡æµ‹è¯• ===")
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # æ£€æŸ¥å…³é”®ç¯å¢ƒå˜é‡
    env_vars = [
        "API_PROVIDER",
        "OPENAI_API_KEY", 
        "OPENAI_BASE_URL",
        "OPENAI_MODEL"
    ]
    
    for var in env_vars:
        value = os.getenv(var)
        if value:
            if "API_KEY" in var:
                # éšè—APIå¯†é’¥çš„å¤§éƒ¨åˆ†å†…å®¹
                display_value = f"{value[:10]}...{value[-4:]}"
            else:
                display_value = value
            print(f"âœ… {var}: {display_value}")
        else:
            print(f"âŒ {var}: Not Set")
    
    print("\n=== é…ç½®ç±»æµ‹è¯• ===")
    
    # æµ‹è¯•é…ç½®ç±»
    try:
        import sys
        from pathlib import Path
        
        # æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
        src_path = Path(__file__).parent / "src"
        sys.path.insert(0, str(src_path))
        
        from agent.configuration import Configuration
        from agent.llm_factory import LLMFactory
        
        config = Configuration.from_runnable_config()
        
        print(f"âœ… API Provider: {config.api_provider}")
        print(f"âœ… Effective Provider: {config.get_effective_api_provider()}")
        
        if hasattr(config, 'openai_model') and config.openai_model:
            print(f"âœ… OpenAI Model: {config.openai_model}")
        
        # éªŒè¯APIå¯†é’¥
        provider = config.get_effective_api_provider()
        if LLMFactory.validate_api_key(provider):
            print(f"âœ… {provider.upper()} API Key is valid")
        else:
            print(f"âŒ {provider.upper()} API Key is missing or invalid")
            
        print("\n=== LLMåˆ›å»ºæµ‹è¯• ===")
        
        # å°è¯•åˆ›å»ºLLM
        llm = LLMFactory.create_llm(
            config=config,
            model_type="query_generator_model"
        )
        
        print(f"âœ… LLM created successfully: {type(llm).__name__}")
        print(f"âœ… Model name: {llm.model}")
        
        return True
        
    except Exception as e:
        print(f"âŒ é…ç½®æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_env_loading()
    
    if success:
        print("\nğŸ‰ æ‰€æœ‰ç¯å¢ƒå˜é‡å’Œé…ç½®éƒ½æ­£å¸¸ï¼")
    else:
        print("\nâŒ é…ç½®æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡")
